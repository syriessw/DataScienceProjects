{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Classification of JIRA comments to find commonality in defects"
      ],
      "metadata": {
        "id": "N4IsE-L8B6E-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The aim of this exercise is to find out if there are any common factors between the raised defects that can be focused on, which can then lead to improvement in quality.\n",
        "\n",
        "The general idea is to obtain comments of all flagged defects from relevant Projects through API, used Pandas to store them and then utilise NLP to see if classification can be done.\n",
        "\n",
        "As Atlassian JIRA store comments under each issue, and there are multiples issues being flagged, there needs to be repeated calls to the API. Considering the size, and time taken for each API call, time can be cut down for the exercise by utilising multi-threading."
      ],
      "metadata": {
        "id": "XBbkIhjKChlZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Set Up"
      ],
      "metadata": {
        "id": "gBrS88NrDB7R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install atlassian-python-api"
      ],
      "metadata": {
        "id": "7eE5wDMcDHjH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install XlsxWriter # For writing to Excel"
      ],
      "metadata": {
        "id": "qnbFcfgsDUPY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JyrmMnLYBs2k"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime, timedelta, timezone\n",
        "from atlassian import JIRA\n",
        "import requests\n",
        "import json\n",
        "\n",
        "# For running threading to make operations run faster\n",
        "import concurrent\n",
        "from concurrent.futures import ThreadPoolExecutor"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from bs4 import BeautifulSoup\n",
        "!pip install lxml\n",
        "import lxml\n",
        "\n",
        "# import reges\n",
        "import re\n",
        "\n",
        "# NLTK package\n",
        "import string\n",
        "from collections import Counter\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize, regexp_tokenize"
      ],
      "metadata": {
        "id": "Erox1b9cEI7C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set credentials\n",
        "cred_auth = {\n",
        "    \"Username\": \"someuser@email.com\", # JIRA Account ID\n",
        "    \"Token\": \"SomeAPIToken\", # JIRA Personal Access Token\n",
        "    \"Encoded_Pass\": \"SomeBase64EncodedToken\" # JIRA username:token Base64 encoding\n",
        "}\n",
        "\n",
        "# Declare jira instance\n",
        "jira_instance = JIRA(\n",
        "    url = \"https://jirainstance.com/\",\n",
        "    token = cred_auth['Token']\n",
        ")\n",
        "\n",
        "# Set query string\n",
        "searchApi_string = \"https://jirainstance.net/rest/api/2/search?jql=\""
      ],
      "metadata": {
        "id": "58yxkgl9DX4h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Methods"
      ],
      "metadata": {
        "id": "Orh3V3wpEvG_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def writeDFToExcel(df, filename):\n",
        "  '''\n",
        "  Method to write to excel based on given dataframe\n",
        "  Also use xlsx writer to set the column width so that the document doesn't open at fixed width\n",
        "  And hide some columns that can be useful\n",
        "  '''\n",
        "  with pd.ExcelWriter(filename, engine='xlsxwriter') as writer:\n",
        "    df.to_excel(writer, sheet_name='Report', encoding='utf-8-sig', index=False)\n",
        "    worksheet = writer.sheets['Report']"
      ],
      "metadata": {
        "id": "BhBnMqn5EzTg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_all_mainquery_results_api(auth, fieldlist, query_string: str, user=None)->list:\n",
        "  '''\n",
        "  Method to retrieve all requests from a specific JIRA JQL request\n",
        "  Not restricted by the page limit count\n",
        "  By taking in the credentials, any fields filter, the query string\n",
        "  and outputting in a dataframe the result of the JSON response\n",
        "  '''\n",
        "  issues_per_query = 100\n",
        "  list_of_jira_issues = []\n",
        "\n",
        "  # Get the total issues in the results set. This is one extra request\n",
        "  num_issues_in_query_result_set = jira_instance.jql(query_string, limit=0)['total']\n",
        "  print(f\"Query {query_string} returns {num_issues_in_query_result_set} issues.\")\n",
        "\n",
        "  queryString_utf = query_string.replace(\" \", \"+\").replace(\"'\", \"%22\") # Replace all spaces and inverted commans in URL string\n",
        "\n",
        "  # Use floor division + 1 to calculate the number of requests needed\n",
        "  for query_number in range(0, (num_issues_in_query_result_set // issues_per_query) + 1):\n",
        "    startNum = query_number * issues_per_query\n",
        "    results = requests.get(\n",
        "        url = \"https://jirainstance.com/rest/api/2/search?jql=\" + queryString_utf,\n",
        "        headers = {\n",
        "            \"Authorization\": \"Bearer \" + cred_auth[\"Token\"],\n",
        "            \"Content-Type\": \"application/json\"\n",
        "        },\n",
        "        params = {'fields': fieldlist, 'maxResults': issues_per_query, 'startAt': startNum}\n",
        "    )\n",
        "\n",
        "    project_issueList = json.loads(results.text)\n",
        "    list_of_jira_issues.extend(project_issueList[\"issues\"])\n",
        "    print(startNum, issues_per_query * (query_number+1)) # Print out which iteration of the loop this is in\n",
        "\n",
        "  df = pd.json_normalize(list_of_jira_issues) # Json serialise into dataframe\n",
        "\n",
        "  # Define fields of interest\n",
        "  fields_of_interest = [\"fields.project.name\", \"fields.issuetype.name\"]\n",
        "\n",
        "  # Filter only to display fields of interest\n",
        "  df_filtered = df.loc[:, df.columns.intersection(fields_of_interest)]\n",
        "\n",
        "  if user != None:\n",
        "    df_filtered['Actioned By'] = user # Create column to specify who was the one who actioned\n",
        "\n",
        "  return df_filtered"
      ],
      "metadata": {
        "id": "4wPo__VsFKSF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_comment_results_api(auth, issue_id)-> list:\n",
        "  '''\n",
        "  Method to get all the comments within a JIRA issue\n",
        "  Takes in the credentials and the issue ID of a JIRA e.g. AKE-1212\n",
        "  Returns a dataframe of all comments\n",
        "  '''\n",
        "  issues_per_query = 100\n",
        "  list_of_comments = []\n",
        "\n",
        "  # Get the total issues in the results set. This is one extra request\n",
        "  num_issues_in_query_result_set = jira_instance.issue_get_comments(issue_id)[\"total\"]\n",
        "\n",
        "  # Use floor division + 1 to calculate the number of requests needed\n",
        "  for query_number in range(0, (num_issues_in_query_result_set // issues_per_query) + 1):\n",
        "    startNum = query_number * issues_per_query\n",
        "    results = requests.get(\n",
        "        url = \"https://jirainstance.com/rest/api/2/issue/\" + issue_id + \"/comment\",\n",
        "        headers = {\n",
        "            \"Authorization\": \"Bearer \" + cred_auth[\"Token\"],\n",
        "            \"Content-Type\": \"application/json\"\n",
        "        },\n",
        "        params = {'startAt': startNum}\n",
        "    )\n",
        "\n",
        "    comment_list = json.loads(results.text)\n",
        "    list_of_comments.extend(comment_list[\"comments\"])\n",
        "\n",
        "  comment_df = pd.json_normalize(list_of_comments) # Json serialise into dataframe\n",
        "\n",
        "  # declare new sequence to store cleaned text of comments\n",
        "  cleanedText_df = []\n",
        "\n",
        "  # Use BeautifulSoup to parse the html text and remove tags / strings\n",
        "  for idx, commentRow in enumerate(comment_df['body']):\n",
        "    soup = BeautifulSoup(commentRow, 'lxml')\n",
        "\n",
        "    # Strip of tags and data decompose\n",
        "    for data in soup(['style', 'script', 'code']):\n",
        "      data.decompose() # remove tags\n",
        "\n",
        "    textList = list(soup.stripped_strings)\n",
        "\n",
        "    # Remove midstring trailing spaces and unicode spaces\n",
        "    for i, content in enumerate(textList):\n",
        "      textList[i] = content.replace('\\r\\n', ' ').replace('\\xa0', ' ')\n",
        "\n",
        "    cleanedText_df.append(' '.join(textList))\n",
        "\n",
        "  # Concatenate the dataframes together\n",
        "  final_df = pd.concat([comment_df[['author.displayName', 'created', 'updated']], pd.DataFrame({'commentCleaned': cleanedText_df})], axis=1)\n",
        "  final_df['updated'] = final_df['updated'].apply(lambda x: datetime.strptime(x, '%Y-%m-%dT%H:%M:%S.%f%z'))\n",
        "  final_df['updated'] = pd.to_datetime(final_df['updated'], utc=True)\n",
        "  final_df['updatedDateOnly'] = final_df['updated'].dt.date\n",
        "  final_df['MainIssue'] = issue_id\n",
        "\n",
        "  return final_df\n"
      ],
      "metadata": {
        "id": "6ablDGnNIhzM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Set up Date Range"
      ],
      "metadata": {
        "id": "Vu58sU9ILHKK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import date\n",
        "from dateutil.relativedelta import relativedelta\n",
        "from pandas.tseries.offsets import MonthEnd\n",
        "set_days_ago = 7\n",
        "\n",
        "today_date = date.today()\n",
        "\n",
        "# Calculate 7 days ago from today\n",
        "seven_days_start = today_date + relativedelta(days= -set_days_ago)\n",
        "timeStart_date = date(seven_days_start.year, seven_days_start.month, seven_days_start.day)\n",
        "timeEnd_date = date(today_date.year, today_date.month, today_date.day)\n",
        "print(\"Starting Date \" + str(timeStart_date) + \"\\nEnding Date: \" + str(timeEnd_date))\n",
        "\n",
        "# Claculate last week start and end\n",
        "start_date = today_date + timedelta(-today_date.weekday(), weeks=-1)\n",
        "end_date = today_date + timedelta(-today_date.weekday() - 1)\n",
        "\n",
        "print(\"Last Week Start \" + str(start_date) + \"\\nLast Week End: \" + str(end_date))"
      ],
      "metadata": {
        "id": "k8VAIO7ULKG2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Retrieve Issues DataFrame by list of users"
      ],
      "metadata": {
        "id": "YJgkuIXXMBLH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "eeList = ['EmployeeName', 'EmployeeName2', 'EmployeeName3']\n",
        "\n",
        "queryString = \"issue in updatedBy({}, '\" + start_date.strftime(\"%Y/%m/%d\") + \"', '\" + end_date.strftime(\"%Y/%m/%d\") + \"')\""
      ],
      "metadata": {
        "id": "JNkFfTEIL7wq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get list of issues updated by the members, if any, within the stipulated time\n",
        "\n",
        "with ThreadPoolExecutor(max_workers=30) as executor:\n",
        "  # With each row in the Dataframe, we set as a future in the thread\n",
        "  futures = [\n",
        "      executor.submit(get_all_mainquery_results_api, cred_auth, field_final, queryString.format(\"'\" + user + \"'\"), user) for user in eeList\n",
        "  ]\n",
        "  concurrent.futures.wait(futures)\n",
        "\n",
        "  df_MainTasksList = pd.DataFrame({'key': [], 'Summary': [], 'Deployed Date': [], 'Description': [], 'Assignee': [], 'country_json': []})\n",
        "\n",
        "  for f in concurrent.futures.as_completed(futures):\n",
        "    try:\n",
        "      data = f.result()\n",
        "    except Exception as exc:\n",
        "      print('%r generated an exception: %s' % (data, exc))\n",
        "    else:\n",
        "      if (not(data.empty)):\n",
        "        df_MainTasksList = pd.concat([df_MainTasksList, data])"
      ],
      "metadata": {
        "id": "UHFAA7pwMp5u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data cleaning"
      ],
      "metadata": {
        "id": "HmFTxVR5OKe_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop duplicates\n",
        "df_finalMainSet = df_MainTasksList.drop_duplicates(subset=['key']).reset_index(drop=True)"
      ],
      "metadata": {
        "id": "USJMABRnOKFE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Replace label column into concatenated string\n",
        "countryCol = []\n",
        "\n",
        "for index, row in df_finalMainSet.iterrows():\n",
        "  country = []\n",
        "\n",
        "  if row['country_json'] is None:\n",
        "    countryCol.append(None)\n",
        "  else:\n",
        "    for countryEntry in row['country_json']:\n",
        "      country.append(countryEntry['value'])\n",
        "\n",
        "    countryString = ','.join(country)\n",
        "    countryCol.append(countryString)"
      ],
      "metadata": {
        "id": "cfsIWYlSOYdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_finalMainSet.insert(6, 'country', countryCol)\n",
        "df_finalMainSet = df_finalMainSet.drop(columns=['country_json'])"
      ],
      "metadata": {
        "id": "GoLSm84dOw5y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Retrieve Comments Dataframe from list of Issues"
      ],
      "metadata": {
        "id": "-kW0xiIrO8gw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get list of comments for every issues\n",
        "with ThreadPoolExecutor(max_workers=30) as execturo:\n",
        "  # with each row in the DataFrame, set as future in thread\n",
        "  futures = [\n",
        "      executor.submit(get_comment_results_api, cred_auth, row['key'] for idx,row in df_finalMainSet.iterrows())\n",
        "  ]\n",
        "  concurrent.futures.wait(futures)\n",
        "\n",
        "  df_completeCommentList = pd.DataFrame({'MainIssue': [], 'author.displayName': [], 'created': [], 'updated': [], 'commentCleaned': [], 'UpdatedDateOnly': []})\n",
        "\n",
        "  for f in concurrent.futures.as_completed(futures):\n",
        "    try:\n",
        "      data = f.result()\n",
        "    except Exception as exc:\n",
        "      print('%r generated an exception: %s' % (data,exc))\n",
        "    else:\n",
        "      if not(data.empty):\n",
        "        df_completeCommentList = pd.concat([df_completeCommentList, data])"
      ],
      "metadata": {
        "id": "sF6IEqSJO_9X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Cleaning round 2"
      ],
      "metadata": {
        "id": "z143b2ikQfTs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Merge with original issues to identify comments to issue\n",
        "df_workOn = pd.merge(df_completeCommentList, df_finalMainSet[['key', 'Description']], how='inner', left_on='MainIssue', right_on='key').drop_duplicates(subset=['key']).reset_index(drop=True)\n",
        "\n",
        "# Remove references to PR\n",
        "df_workOn = df_workOn[df_workOn['commentCleaned'].str.contains('tfs')]\n",
        "\n",
        "# Remove all other duplicates of same issue since only interested in the first instance\n",
        "df_workOn.sort_values(by=['UpdatedDateOnly'], inplace=True)\n",
        "df_workOn.drop_duplicates(subset=['MainIssue', 'author.displayName'], inplace=True)"
      ],
      "metadata": {
        "id": "3rWeFyV3QFLK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## NLTK Preprocessing"
      ],
      "metadata": {
        "id": "Sd5QSXeMRXs3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Start with Preprocessing\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('vader_lexicon')\n",
        "\n",
        "# Initialise the text preprocessing tools\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stop_words = set(stopwords.words('english'))"
      ],
      "metadata": {
        "id": "bUQ6Kel7RaJc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_TFStext(text):\n",
        "  '''\n",
        "  Method taking in the comments and tokenizing through regex\n",
        "  This identifies if the string with overview exists which represents\n",
        "  the comment is for a PR\n",
        "  '''\n",
        "  newToken = ''\n",
        "  tokens = regexp_tokenize(text, pattern=r\"\\s|.,;'\", gaps=True) # Split out words by the given delimiters\n",
        "  alphaOrPunc = [word for word in tokens is word.isalpha() or word in string.punctuation or word.isnumeric()]\n",
        "  remain = [word for word in tokens if word not in alphaOrPunc] # Only keep tokenize words that are not alpha or puncutation or numeric\n",
        "  remain = [identify_tfs(word) for word in remain] # Keep\n",
        "  remain = [word for word in remain if word] # remove empty strings from list\n",
        "\n",
        "  # When stripping html tags, whitespaces were removed leading to mash-up of words, so try to split out again\n",
        "  # Check that overview is the last possible word\n",
        "  for index, word in enumerate(remain):\n",
        "    if word.find('overview') > 0 and word.find('overview') + 8 < len(word):\n",
        "      remain[index] = word[:word.find('overview') + 8]\n",
        "\n",
        "  final = \",\".join(remain)\n",
        "\n",
        "  if final:\n",
        "    final = final\n",
        "  else:\n",
        "    final = None\n",
        "\n",
        "  return final\n",
        "\n",
        "def identify_tfs(word):\n",
        "  '''\n",
        "  Method that takes in a word and search if there exists \"tfs\"\n",
        "  Return the actual URL text instead of the markdown with square brackets\n",
        "  '''\n",
        "  index = word.find('https://')\n",
        "  html = ''\n",
        "  if index > 0:\n",
        "    word = word[index:]\n",
        "\n",
        "  if re.search('\\w\\/tfs\\w?', word):\n",
        "    html = word.replace('[', '').replace(']', '')\n",
        "  else:\n",
        "    html = ''\n",
        "\n",
        "  return html"
      ],
      "metadata": {
        "id": "c7ptq4D3RwR9"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create new column to store all TFS url\n",
        "df_workOn['TFS_Url'] = df_workOn['commentCleaned'].apply(process_TFStext)"
      ],
      "metadata": {
        "id": "a_-uv3KVR4bX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def removeTFS(urlString, TextString):\n",
        "  '''\n",
        "  Method that takes in both url and the comment string\n",
        "  and returns the comment string without the url\n",
        "  '''\n",
        "  newString = TextString\n",
        "  for url in urlString.split(','):\n",
        "    newString = newString.replace(url, '')\n",
        "\n",
        "  return newString\n",
        "\n",
        "def removeAllSquareBracketText(TextString):\n",
        "  '''\n",
        "  Method that removes all square brackets\n",
        "  and return comment string without it\n",
        "  '''\n",
        "  newString = TextString\n",
        "  matches = re.findall(r'\\[(.*?)\\]', TextString) # Obtain list of text in square brackets\n",
        "  for url in matches:\n",
        "    newString = newString.replace(url, '')\n",
        "\n",
        "  return newString\n",
        "\n",
        "def removeAllJIRATicketReferences(TextString):\n",
        "  '''\n",
        "  Method that takes in comment string\n",
        "  and returns comment string without JIRA ticket\n",
        "  '''\n",
        "  newString = TextString\n",
        "  matches = re.findall(r'[\\w]{4,}-[0-9]{1,}', TextString) # Obtain list of possible JIRA tickets\n",
        "  for url in matches:\n",
        "    newString = newString.replace(url, '')\n",
        "\n",
        "  return newString"
      ],
      "metadata": {
        "id": "aLqGwNQ_b9AY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_workOn['RemainingText'] = df_workOn['commentCleaned']\n",
        "\n",
        "# Remove any links / tags caused by square brackets\n",
        "df_workOn['RemainingText'] = df_workOn['RemainingText'].apply(removeAllSquareBracketText)\n",
        "\n",
        "# Remove TFS URL. Because it is currently in an array, split by delimiter and run across each element against the string\n",
        "df_workOn['RemainingText'] = df_workOn.apply(lambda x: removeTFS(x['TFS URL'], x['RemainingText']), axis=1)\n",
        "\n",
        "# Remove all Jira tickets\n",
        "df_workOn['RemainingText'] = df_workOn['RemainingText'].apply(removeAllJIRATicketReferences)"
      ],
      "metadata": {
        "id": "ZesInHCJcmnJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Text Preprocessing\n",
        "\n",
        "def get_wordnet_pos(treebank_tag):\n",
        "  '''\n",
        "  Method that maps POS tag to first character used by WordNetLemmatizer\n",
        "  '''\n",
        "  if treebank_tag.startswith('J'):\n",
        "    return wordnet.ADJ\n",
        "  elif treebank_tag.startswith('V'):\n",
        "    return wordnet.VERB\n",
        "  elif treebank_tag.startswith('N'):\n",
        "    return wordnet.NOUN\n",
        "  elif treebank_tag.startswith('R'):\n",
        "    return wordnet.ADV\n",
        "  else:\n",
        "    return wordnet.NOUN\n",
        "\n",
        "def remove_html(word):\n",
        "  '''\n",
        "  Method that takes in a word and goes through all checks\n",
        "  replacing without the checks\n",
        "  '''\n",
        "  # print(\"tfs\", word, re.search('\\A\\/tfs\\w?', word))\n",
        "  # print(\"_aoverview\", word, re.search('\\A_a=\\w?', word))\n",
        "  # print(\"special char\", word, re.search('^[^a-zA-Z0-9]*$', word))\n",
        "  # print(\"^ at start\", word, re.search('^[\\^]\\w+', word))\n",
        "  # print(\"check for file cases\", word, re.search('\\w*\\.[a-zA-Z]{3, 4}', word))\n",
        "  # print(\"check for jira tagging via ~\", word, re.search('\\w?~\\w+', word))\n",
        "\n",
        "  if re.search('\\A\\/tfs\\w?', word):\n",
        "    return ''\n",
        "  elif re.search('\\A_a=\\w?', word):\n",
        "    return ''\n",
        "  elif re.search('^[^a-zA-Z0-9]*$', word):\n",
        "    return ''\n",
        "  elif re.search('^[\\^]\\w+', word):\n",
        "    return ''\n",
        "  elif re.search('\\w*\\.[a-zA-Z]{3, 4}', word):\n",
        "    return ''\n",
        "  elif re.search('\\w?~\\w+', word):\n",
        "    return word"
      ],
      "metadata": {
        "id": "hZoM9LPGdu08"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_description(text):\n",
        "  '''\n",
        "  Method taking in the comment string\n",
        "  and preprocessing into simple words\n",
        "  '''\n",
        "  tokens = word_tokenize(text)\n",
        "  tokens=[word.lower() for word in tokens]\n",
        "  tokens = [word for word in tokens if word.isalpha() or word in string.punctuation]\n",
        "  pos_tags = nltk.pos_tag(tokens)\n",
        "  tokens = [lemmatizer.lemmatize(word, get_wordnet_pos(pos)) for word,pos in pos_tags]\n",
        "  tokens = [word for word in tokens if word not in stop_words]\n",
        "\n",
        "  return \" \".join(tokens)\n",
        "\n",
        "df_workOn['DescriptionCleaned'] = df_workOn['Description'].apply(preprocess_description)"
      ],
      "metadata": {
        "id": "rimzkIBufhDZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Find common themes in those that currently fail the test\n",
        "def most_common(df, top_n=10):\n",
        "  list_to_exclude = ['please', 'would', 'team', 'thanks', 'thank', 'hope', 'client']\n",
        "  name_to_exclude = ['tom', 'harry']\n",
        "  all_words = [word for word in ' '.join(df['DescriptionCleaned']).split() if word not in string.puncutation and word not in list_to_exclude and word not in name_to_exclude]\n",
        "  word_counts = Counter(all_words)\n",
        "  top_10_words = word_counts.most_common(top_n)\n",
        "\n",
        "  return top_10_words\n",
        "\n",
        "list_top10 = most_common(df_workOn)\n",
        "\n",
        "def findWithinTop10(TextString):\n",
        "  # for each top10 words, compare against the list and see if it's within\n",
        "  reasonsList = []\n",
        "  for word in list_top10:\n",
        "    if re.search(word[0], TextString):\n",
        "      reasonsList.append(word[0])\n",
        "\n",
        "  return \",\".join(reasonsList)"
      ],
      "metadata": {
        "id": "ESOFmuaBgIbs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_workOn['PossibleReason'] = df_workOn['DescriptionCleaned'].apply(findWithinTop10)"
      ],
      "metadata": {
        "id": "3c0_ysw7hABM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Export"
      ],
      "metadata": {
        "id": "SMI76Lt0hKm0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Final clean-up\n",
        "df_workOn = df_workOn.drop(columns=['index', 'RemainingText', 'DescriptionCleaned'])\n",
        "df_workOn = df_workOn.rename(columns={'author.displayName': 'Author', 'commentCleaned': 'OriginalComment', 'UpdatedDateOnly': 'Date'})"
      ],
      "metadata": {
        "id": "q9D69ur-hLaT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "timestamp_str = start_date.strftime('%Y%m%d')\n",
        "\n",
        "filename = timestamp_str + '_CommentsCleanedJIRA' + '.xlsx'\n",
        "\n",
        "writeDFToExcel(df_workOn, filename)"
      ],
      "metadata": {
        "id": "IOPaLAhlheg6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Summary"
      ],
      "metadata": {
        "id": "LKKyCKW4hxYD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Through the NLTK package, I can tokenize the comments and find the 10 most common bag of words and then compare it against individual comment to classify the reasons.\n",
        "\n",
        "Problems I ran into includes how raw the JIRA data is and how much cleaning I had to do before the comments are considered ready. Furthermore, as this involves comments, there are also instances of names and markdown that needs to be carefully removed. A lot of Regex was utilised on the comment strings to slowly strip the excess information, which took a lot of time.\n",
        "\n",
        "Overall though, the project was able to highlight about 40% of the issues correctly after reviewing which is not super high, but enough to find out there is some patterns in the defects being raised."
      ],
      "metadata": {
        "id": "o68Kz78uh09o"
      }
    }
  ]
}